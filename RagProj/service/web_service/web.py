import json
import time
from tempfile import NamedTemporaryFile
import os

import streamlit as st
import argparse
from flag_models import FlagModel
from transformers import AutoTokenizer, AutoModel

from streamlit_chat import message
from split import load_file
import numpy as np
import faiss

# langchain embedding
st.set_page_config(page_title="åŸºäºçŸ¥è¯†åº“çš„å¤§å‹è¯­è¨€æ¨¡å‹é—®ç­”")
st.title("åŸºäºçŸ¥è¯†åº“çš„å¤§å‹è¯­è¨€æ¨¡å‹é—®ç­”")


def get_args():
    parser = argparse.ArgumentParser(description='data helper')
    parser.add_argument('--embed_model_path', type=str, help='å‘é‡è¡¨å¾æ¨¡å‹è·¯å¾„')
    parser.add_argument('--model_path', type=str, default="./", help="å¤§æ¨¡å‹è·¯å¾„")

    return parser.parse_args()


class EmbeddingService:
    def __init__(self, args):
        self.embed_model_path = args.embed_model_path
        self.embed_model = self.load_embedding_model(self.embed_model_path)

    def load_embedding_model(self, model_path):
        embed_model = FlagModel(model_path,
                                query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                                use_fp16=True)
        return embed_model

    def get_embedding(self, doc_info):
        doc_vectors = self.embed_model.encode(doc_info)
        doc_vectors = np.stack(doc_vectors).astype('float32')
        dimension = 512
        index = faiss.IndexFlatL2(dimension)
        index.add(doc_vectors)
        return index

    def search_doc(self, query, doc_info, index: faiss.IndexFlatL2, k: int):
        query_vector = self.embed_model.encode([query])
        query_vector = np.array(query_vector).astype('float32')
        distances, indexes = index.search(query_vector, k)
        found_docs = []
        for i, (distance, index) in enumerate(zip(distances[0], indexes[0])):
            print(f"Result {i + 1}, Distance: {distance}")
            found_docs.append(doc_info[i])
        return found_docs


class GLMService:
    def __init__(self, args):
        self.args = args
        self.glm_model, self.tokenizer = self.init_model(self.args.model_path)

    def init_model(self, model_path):
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()
        model = model.eval()
        return model, tokenizer

    def get_result(self, doc, question):
        input_ = f"ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®æ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”çš„æœºå™¨äººï¼Œä»¥ä¸‹æ˜¯ç”¨äºå‚è€ƒçš„æ–‡æ¡£å†…å®¹ï¼š\n\n{doc}\né—®é¢˜ä¸ºï¼š{question}\nç­”ï¼š"
        response, history = self.glm_model.chat(self.tokenizer, input_, history=[])
        return response


def clear_chat_history():
    del st.session_state.messages
    st.session_state.history = []


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


# åˆå§‹åŒ–å˜é‡
if 'history' not in st.session_state:
    st.session_state.history = []

# åˆå§‹åŒ– session_state
if "enter_pressed" not in st.session_state:
    st.session_state.enter_pressed = False


def generate_response(user_input):
    info = "æ ¹æ®åŸæ–‡â€œå­©å­ä»¬å¾ˆå®¹æ˜“åœ¨æ‰‹æœ¯å‰æ„Ÿåˆ°ç´§å¼ æˆ–ç„¦è™‘,å› ä¸ºä»–ä»¬ä¸å¤ªå®¹æ˜“ç†è§£å¤æ‚çš„åŒ»å­¦æœ¯è¯­ã€‚æ­¤å¤–,åŒ»ç”Ÿä¹Ÿä¸æ„¿æ„å‘16å²ä»¥ä¸‹çš„å„¿ç«¥å¼€æŠ—ç„¦è™‘è¯ç‰©ã€‚â€å¯çŸ¥ï¼Œå­©å­ä»¬å®¹æ˜“åœ¨æ‰‹æœ¯å‰æ„Ÿåˆ°ç´§å¼ æˆ–ç„¦è™‘æ˜¯å› ä¸ºä»–ä»¬ä¸å¤ªå®¹æ˜“ç†è§£å¤æ‚çš„åŒ»å­¦æœ¯è¯­ï¼Œè€Œä¸”åŒ»ç”Ÿä¹Ÿä¸æ„¿æ„å‘16å²ä»¥ä¸‹çš„å„¿ç«¥å¼€æŠ—ç„¦è™‘è¯ç‰©ã€‚"
    return info


def main():
    # åˆå§‹åŒ–ChatGLM3-6æ¨¡å‹
    # st.markdown("å¼€å§‹åˆå§‹åŒ–ï¼Œè¯·ç¨å...")
    st.session_state.glm_service = GLMService(args)
    print("llm is loaded")

    # åˆå§‹åŒ–BGEå‘é‡æ¨¡å‹
    st.session_state.embedding_service = EmbeddingService(args)
    print("embedding_service is loaded")

    lines = ["1", "2", "3"]
    st.session_state.embedding_service.get_embedding(lines)
    st.markdown("åˆå§‹åŒ–å®Œæˆï¼Œè¯·ä¸Šä¼ æ–‡ä»¶")
    # è·å–çŸ¥è¯†åº“æ–‡ä»¶
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ æ–‡ä»¶")
    temp_file = NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1])
    temp_file.write(uploaded_file.getvalue())
    # æ„é€ åŒ…å«æ‰©å±•åçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    file_path = temp_file.name
    with st.spinner('Reading file...'):
        texts = load_file(file_path)
    st.success('Finished reading file.')
    temp_file.close()

    # åˆå§‹åŒ–æ–‡æ¡£ï¼Œå¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œå‘é‡åŒ–
    st.session_state.index = st.session_state.embedding_service.get_embedding(texts)

    # è·å–ç”¨æˆ·é—®é¢˜
    st.markdown("#### è¯·åœ¨ä¸‹åˆ—æ–‡æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
    if 'generated' not in st.session_state:
        st.session_state['generated'] = []
    if 'past' not in st.session_state:
        st.session_state['past'] = []
    user_input = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:", key='input')

    if user_input:
        # è·å–ç”¨æˆ·é—®é¢˜ï¼Œå¹¶å¾—åˆ°å‘é‡è¡¨å¾ï¼Œåˆ©ç”¨å·²åŠ è½½çš„Faissè·å–Kä¸ªç›¸å…³doc
        found_docs = st.session_state.embedding_service.search_doc(user_input, texts, st.session_state.index, k=3)
        st.markdown("#### æ£€ç´¢åˆ°ä»¥ä¸‹å†…å®¹ï¼š")
        for one in found_docs:
            st.markdown(f"{one}")

        found_doc = "\n".join(found_docs)

        # ç”Ÿæˆç­”æ¡ˆå¹¶è¿”å›ç»“æœå±•ç¤º
        output = st.session_state.glm_service.get_result(found_doc, user_input)
        st.session_state['past'].append(user_input)
        st.session_state['generated'].append(output)
    if st.session_state['generated']:
        for i in range(len(st.session_state['generated']) - 1, -1, -1):
            message(st.session_state["generated"][i], key=str(i))
            message(st.session_state['past'][i],
                    is_user=True, key=str(i) + '_user')
            st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    args = get_args()
    main()
