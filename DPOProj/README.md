本项目为书籍《大型语言模型实战指南：应用实践与场景落地》中第3章《基于DPO的偏好对齐实战》实战部分代码-DPO任务实战。

## 项目简介
针对知乎问答的偏好对齐数据集，运用DPO算法对生成结果进行干预，使其生成结果更具有点赞倾向。

项目主要结构如下：
- data：对齐数据集
    -  zhihu_3k_rlfh.tsv
- model：模型文件夹
- data_set.py：模型所需数据类文件
- train.py：模型训练文件

注意：由于GitHub不方便放模型文件，因此model文件夹中的模型bin文件，请从百度云盘中下载。

| 文件名称 | 下载地址 | 提取码 |
| --- |--- |---|
| TinyLlama-1.1B |[百度云](https://pan.baidu.com/s/1GCH0tU9HUG95IdavVxgzHA) |j46b|


## 环境配置
模型训练或推理所需环境，请参考requirements.txt文件。

## 模型训练
模型训练需要运行train.py文件

命令如下：
```shell
python3 train.py --device 0 
```
注意：当服务器资源不同或读者更换数据等时，可以在模型训练时修改响应参数，详细参数说明见代码或阅读书3.3.3小节。


## 总结
本项目中的代码包含大量的注释信息，帮助读者更容易的阅读代码、以及了解其原理。读者跑通代码的后，可以根据自己特定的任务，定向修改配置参数或代码，实现自己响应的功能。